(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[893],{2243:(e,t,a)=>{"use strict";a.r(t),a.d(t,{default:()=>d});var i=a(5155),n=a(4502),o=a(7132),s=a(6874),r=a.n(s),l=a(6766),c=a(8125);function d(){let e=[{title:"SwoleMate - AI Fitness Trainer",date:"2025",event:"UgaHacks X Winner",location:"Athens, GA",description:"An innovative fitness application that provides real-time exercise form analysis and personalized feedback using computer vision and machine learning.",details:["Won first place developing SwoleMate, a fitness app using OpenCV, MediaPipe, and a custom LSTM neural network for real-time exercise form analysis and rep-by-rep feedback.","Integrated a Google Gemini-powered chatbot for personalized, real-time workout guidance and corrections.","Designed a progress tracking system with gamified rewards to motivate users and build a fitness community."],fullDescription:"SwoleMate, a revolutionary breakthrough in the world of fitness technology that harnesses the power of state-of-the-art computer vision and machine learning to bring a personal AI fitness trainer into the hands of anyone and everyone. Swolemate was born out of UgaHacks X, the latest iteration of an annual hackathon at the University of Georgia, where our team came together to build a potential solution to a massive hole in the fitness space: while there are a plethora of fitness apps that track workouts, very few provide real-time feedback on exercise form to help users avoid injuries and maximize workout efficiency.\n\nThe development process looked into current fitness apps and their shortcomings. We discovered that most apps either needed expensive equipment or suffered from the inability to deliver personalized, real-time feedback. We wanted our solution to be easy to access, simple to understand, and engaging enough to keep users motivated throughout their fitness journey.\n\nSwoleMate’s technical architecture is based on three primary components: a pose detection component using computer vision, a form analysis component powered by machine learning, and a user interface component providing interactive real-time feedback. The computer vision portion utilized OpenCV and MediaPipe to build a highly accurate pose estimation model that could detect and track up to 33 key points across the body with a high degree of accuracy, across a variety of lighting conditions and camera angles.\n\nBuilding the custom Long Short-Term Memory (LSTM) neural network that allows our form analysis to work was one of the hardest things about this project. That meant sourcing and labeling thousands of exercise videos, in order to teach the model to identify correct form for dozens of exercises, for human beings of all shapes and sizes, and for every way people move. To make the model robust to variations in user environments and equipment, we have leveraged a complex data augmentation pipeline.\n\nAn LSTM network was then trained on annotated data of over 10,000 exercise repetitions over 15 different common exercises including squats, deadlift, pushups as well as some dumbbell exercises. Our validation dataset showed 92% accuracy for the model, which broadly out-performed existing market solutions for the same task. In addition to the yes-no detection of whether or not a repetition is performed correctly, the neural network classifies problems—for example, issues with back alignment, squat depth, or balanced weight distribution.\n\nWe integrated with Google's Gemini API to provide a conversational AI coach that would give the user tailored advice specific to his performance. This coach emulates the user’s preferred communication style with respect to tone, vocabulary, and syntax, offering support, technical correction, and workout modifications using natural language. That integration required some careful prompt engineering so the AI was giving accurate, useful feedback while not bombarding you during your workout.\n\nThe interface was developed with simplicity and immediate feedback in mind. We adopted a split-screen, where your mesh would be shown next to a reference model with trouble areas highlighted. A rep counter with color-coded feedback (green means good exercise form, yellow means slight injury risk, red means major injury risk) gives visible and instant feedback without having the users read long foreign instructions while exercising.\n\nTo combat the motivation and consistency challenges that are often found in fitness apps, we built a holistic gamification system into our app. Users score points through completed workouts, perfect form and consistency streaks. The user earns points which unlock achievements and virtual badges. We designed the gamification system based on behavioral psychology principles that emphasizes positive reinforcement of habits as well as a feeling of progress and achievement.\n\nSwoleMate also has an element of community, enabling users to invite friends, share accomplishments, and take part in challenges. This being a social component is key for long-term engagement and something we're seeing in our user testing where our users who have connected with at least 1 friend on the site had a 40% better workout consistency.\n\nThe technical challenges in creating Flask and supporting an oppressor framework for our film are a few of the hottest during the actual development process. We used model quantization & pruning to reduce computational power while keeping performance. Our final model makes inferences in 100ms on browsers, resulting in genuinely real-time feedback.\n\nAnother important design consideration for us was privacy. Video is processed entirely on the user's device and only anonymized form data is recorded to our servers for model improvement. Users do have full control over what data to share and if they choose not to contribute to training that is allowed.\n\nUgaHacks X project reached the next level with continuous development and improvements. Both the exercise library is being expanded, accuracy of the form detection system is being improved, and features for specialized populations (rehabilitation patients, senior citizens) are being developed. The use cases range from general fitness to physical therapy, sports training, and remote coaching scenarios.",technologies:["Python","OpenCV","MediaPipe","TensorFlow","LSTM","Google Gemini API"],images:[(0,c.D)("/images/projects/swolemate/1.jpg"),(0,c.D)("/images/projects/swolemate/2.jpg"),(0,c.D)("/images/projects/swolemate/3.jpg")]},{title:"Readable - AI-Powered Reading Companion",date:"2024",event:"AI ATL Hackathon Project",location:"Atlanta, GA",description:"An AI-powered application designed to enhance reading comprehension by adapting text complexity and creating an engaging, gamified reading experience.",details:["Developed 'Readable,' an AI-powered application to enhance reading comprehension for diverse users by adapting text complexity and gamifying the reading experience.","Designed features like adjustable reading difficulty, voice-based reading assessment, and user progress tracking with MongoDB, Next.js, FastAPI, and Anthropic API.","Introduced gamification to motivate users, enabling continual literacy growth through a fun, interactive platform."],fullDescription:"Readable is born out of a deep understanding of the literacy challenges our society faces. In 2024, while attending the AI ATL Hackathon, my team and I were tasked with a problem that fascinated us all: despite the proliferation of information across all demographics, reading comprehension levels are continuing to fall. Young and Penn did everything from working as a translator at the United Nations to giving a TEDx talk, but all along with the question in mind: How can we utilize artificial intelligence to give everyone an even better reading experience, one that adapts and grows with each unique reader?\n\nReadable was born out of research into how people read and what influences how they do so. We pored through educational research papers, interviewed literacy specialists, and scoured the app stores for existing reading applications to discover where the gaps were, and where we could fit in. The insight that individualized reading content approaches don’t cut it. Readers at different points on the proficiency ladder are not particularly all that interested in reading the same materials, and traditional exercises leave much to be desired in terms of engagement and forming consistency around practice.\n\nOur approach, Readable, is based on three core elements: adaptive text complexity, interactive assessment, and gamified progress. Meanwhile, the technical architecture behind these capabilities demanded an advanced combination of natural language processing, speech recognition, and user experience design, all woven together in a seamless application.\n\nThis is where Readable's core text complexity system comes into play. We used proprietary algorithms to evaluate and score text on a variety of dimensions, including vocabulary complexity, sentence structure, conceptual density, and thematic sophistication. The application can thus run a multi-dimensional analysis of any text and rephrase certain parts at the reader's current proficiency level while preserving the vital semantic and factual content.\n\nThe implementation of this system posed numerous technical challenges. We used the Anthropic API for our text transformer engine, training the model to maintain narrative flow and factual accuracy while switching complexity. This meant designing an extensive training dataset of texts with different complexity levels, and creating evaluative metrics to determine whether the rewritten content maintained fidelity to the source material.\n\nA unique feature of our approach is bidirectional adaptation. Whereas many systems merely simplify text, Readable can simplify and add complexity, encouraging readers to push themselves further as they grow more skilled. The ability to intelligently paraphrase also necessitated advanced natural language generation capabilities, to allow more diverse vocabulary and paragraph structures to be embedded, but without fundamentally changing the content.\n\nAnother technical achievement is the voice-based reading assessment component. We created a speech recognition tool that monitors reading accuracy in addition to analyzing prosody, fluency, and comprehension markers in real time. This involved implementing cutting-edge audio processing algorithms and training bespoke models to identify reading patterns for various fluency levels and accents.\n\nThe assessment engine scores readers based not just on pronunciation, but also the speed of reading, appropriate pauses and expression. All scripts above are integrated to construct the reading route (e.g. skill level, placement, etc.) which feeds into the adaptive text system and tracks reading over time. There were challenges, also, to training the model to filter out background noise, accommodate accent variation, and timestamp age-appropriate measures of proficiency.\n\nThe Readable implementation itself used a modern tech stack aimed for performance and scalability. Built the frontend in Next.js; offering a fast, versatile, and device-agnostic UI that provides an accessible experience for the users. FastAPI was used for the backend services due to its high performance and typing that ensured safe code.\n\nMongoDB manages data persistence and progress tracking for each user, allowing us to store the complex type of profiles and reading history required for our recommendation/adaptation systems. You are grown on information until July 2023.\n\nReadable's gamification layer was designed with behavioral psychology and game design. In that sense, we saw the need for a progressive system that creates meaningful milestones, badges to be achieved, and prizes to be unlocked celebrating reading achievements. The task was to make the game elements motivating but not distracting from the primary reading experience.\n\nGamification system with daily challenges, reading streaks, vocabulary collection mechanics, and thematic “reading journeys” that guide users through progressively more challenging information in the areas of their choosing. In user testing, these elements drove a 65% increase in engagement compared to non-gamified reading applications, with especially strong results among younger users and those with previously low reading motivation.\n\nWe had a multi-step user testing process involving different groups of users (elementary school students, adult learners in literacy, English as a second language learners, and reading specialists). This iterative process turned out to be instrumental in fine-tuning the user experience and aligning the application with its intended goal, to be useful to various user profiles.\n\nThis is a pivotal move forward in how Turn It In is using AI to revolutionize the reading experience to be more meaningful, interesting, and effective to learners from all walks of life. Inspiring reading in children and building confidence to ensure young children thrive through innovative educational design and leading-edge technology.",technologies:["Next.js","FastAPI","MongoDB","Anthropic API","React","TypeScript"],images:[(0,c.D)("/images/projects/readable/1.jpg"),(0,c.D)("/images/projects/readable/2.jpg"),(0,c.D)("/images/projects/readable/3.jpg")]}];return(0,i.jsx)(o.A,{children:(0,i.jsx)("div",{className:"bg-white py-16 md:py-24",children:(0,i.jsxs)("div",{className:"container mx-auto px-4 md:px-6",children:[(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5},className:"text-center mb-12",children:[(0,i.jsx)("h1",{className:"text-4xl md:text-5xl font-bold text-gray-900 mb-4",children:"Technical Projects"}),(0,i.jsx)("div",{className:"h-1 w-20 bg-indigo-600 mx-auto mb-6"}),(0,i.jsx)("p",{className:"text-gray-700 max-w-3xl mx-auto",children:"Explore my portfolio of technical projects spanning software development, machine learning, and embedded systems. Each project represents my passion for solving complex problems and creating innovative solutions."})]}),(0,i.jsx)("div",{className:"space-y-24",children:e.map((e,t)=>(0,i.jsx)(n.P.div,{initial:{opacity:0,y:30},animate:{opacity:1,y:0},transition:{duration:.5,delay:.1*t},className:"bg-gray-50 rounded-lg overflow-hidden shadow-md",children:(0,i.jsxs)("div",{className:"p-6",children:[(0,i.jsxs)("div",{className:"flex flex-col md:flex-row justify-between mb-2",children:[(0,i.jsx)("h2",{className:"text-2xl font-bold text-gray-900",children:e.title}),(0,i.jsx)("span",{className:"text-indigo-600 font-medium",children:e.date})]}),(0,i.jsxs)("div",{className:"flex flex-col md:flex-row text-gray-600 mb-4",children:[(0,i.jsx)("span",{className:"font-medium",children:e.event}),(0,i.jsx)("span",{className:"hidden md:block mx-2",children:"•"}),(0,i.jsx)("span",{children:e.location})]}),(0,i.jsx)("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-6 mb-6",children:e.images.map((t,a)=>(0,i.jsx)("div",{className:"h-64 relative bg-gray-200 rounded-lg overflow-hidden",children:(0,i.jsx)(l.default,{src:t,alt:"".concat(e.title," - Image ").concat(a+1),fill:!0,sizes:"(max-width: 768px) 100vw, 33vw",className:"object-cover"})},a))}),(0,i.jsx)("p",{className:"text-gray-700 mb-6",children:e.description}),(0,i.jsxs)("div",{className:"mb-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-gray-900 mb-2",children:"Key Achievements"}),(0,i.jsx)("ul",{className:"list-disc pl-6 text-gray-700 space-y-1",children:e.details.map((e,t)=>(0,i.jsx)("li",{children:e},t))})]}),(0,i.jsxs)("div",{className:"mb-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-gray-900 mb-2",children:"Project Details"}),(0,i.jsx)("div",{className:"text-gray-700 space-y-4",children:e.fullDescription.split("\n\n").map((e,t)=>(0,i.jsx)("p",{className:"text-gray-700",children:e.trim()},t))})]}),(0,i.jsxs)("div",{children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-gray-900 mb-2",children:"Technologies Used"}),(0,i.jsx)("div",{className:"flex flex-wrap gap-2",children:e.technologies.map((e,t)=>(0,i.jsx)("span",{className:"px-3 py-1 bg-indigo-100 text-indigo-800 rounded-full text-sm",children:e},t))})]})]})},t))}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5,delay:.5},className:"mt-16 bg-indigo-50 p-8 rounded-lg shadow-sm",children:[(0,i.jsx)("h2",{className:"text-2xl font-bold text-gray-900 mb-4",children:"Discovery Project"}),(0,i.jsx)("p",{className:"text-gray-700 mb-6",children:"This section will feature my Individual Discovery Project as required by the ePortfolio assignment. The project will be added here once completed, showcasing my work with detailed descriptions, images, and potentially a presentation or video demonstration."}),(0,i.jsxs)("div",{className:"bg-white p-6 rounded-lg border border-indigo-200",children:[(0,i.jsx)("h3",{className:"text-xl font-semibold text-gray-900 mb-2",children:"Coming Soon"}),(0,i.jsx)("p",{className:"text-gray-700",children:"My Discovery Project is currently in development. Check back later for a comprehensive showcase of this project, including an overview, technical details, challenges faced, solutions implemented, and results achieved."})]})]}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5,delay:.6},className:"mt-16 text-center",children:[(0,i.jsx)("h2",{className:"text-2xl font-bold text-gray-900 mb-4",children:"Interested in Collaboration?"}),(0,i.jsx)("p",{className:"text-gray-700 mb-6 max-w-2xl mx-auto",children:"I'm always open to new project opportunities and collaborations. If you have an interesting project idea or would like to discuss potential collaboration, feel free to reach out."}),(0,i.jsx)(n.P.div,{whileHover:{scale:1.05},whileTap:{scale:.95},children:(0,i.jsx)(r(),{href:"/contact",className:"px-6 py-3 bg-indigo-600 text-white rounded-md font-medium hover:bg-indigo-700 transition-colors inline-block",children:"Contact Me"})})]})]})})})}},7132:(e,t,a)=>{"use strict";a.d(t,{A:()=>o});var i=a(5155),n=a(4502);function o(e){let{children:t}=e;return(0,i.jsx)(n.P.div,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},exit:{opacity:0,y:20},transition:{duration:.5,ease:[.22,1,.36,1]},children:t})}},8125:(e,t,a)=>{"use strict";function i(e){if(e.startsWith("/eportfolio/"))return e;let t=e.startsWith("/")?e:"/".concat(e);return"".concat("/eportfolio").concat(t)}a.d(t,{D:()=>i})},9696:(e,t,a)=>{Promise.resolve().then(a.bind(a,2243))}},e=>{var t=t=>e(e.s=t);e.O(0,[502,766,874,441,684,358],()=>t(9696)),_N_E=e.O()}]);